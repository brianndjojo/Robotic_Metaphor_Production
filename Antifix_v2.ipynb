{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c56895f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Requirement already satisfied: torch==1.10.1+cu113 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (1.10.1+cu113)\n",
      "Requirement already satisfied: torchvision==0.11.2+cu113 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (0.11.2+cu113)\n",
      "Requirement already satisfied: torchaudio===0.10.1+cu113 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (0.10.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from torch==1.10.1+cu113) (4.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from torchvision==0.11.2+cu113) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from torchvision==0.11.2+cu113) (9.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch\n",
    "!pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862ccfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: requests in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers requests beautifulsoup4 pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c5ee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysimplegui in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (4.60.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pysimplegui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce74180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\brian\\documents\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# NLTK: Natural Language Toolkit used for Natural Language Processing\n",
    "# https://studymachinelearning.com/an-introduction-to-n-grams/\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdecbde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting more-itertools\n",
      "  Downloading more_itertools-8.13.0-py3-none-any.whl (51 kB)\n",
      "Installing collected packages: more-itertools\n",
      "Successfully installed more-itertools-8.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install more-itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c48cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install NLTK resources\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ac4668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- --------------------\n",
      "aiohttp                       3.8.1\n",
      "aiosignal                     1.2.0\n",
      "alabaster                     0.7.12\n",
      "anaconda-client               1.9.0\n",
      "anaconda-navigator            2.1.4\n",
      "anaconda-project              0.10.2\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.2\n",
      "astroid                       2.6.6\n",
      "astropy                       5.0.4\n",
      "asttokens                     2.0.5\n",
      "async-timeout                 4.0.1\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         21.4.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "Babel                         2.9.1\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.11.1\n",
      "binaryornot                   0.4.4\n",
      "bitarray                      2.4.1\n",
      "bkcharts                      0.2\n",
      "black                         19.10b0\n",
      "bleach                        4.1.0\n",
      "bokeh                         2.4.2\n",
      "boto3                         1.21.32\n",
      "botocore                      1.24.32\n",
      "Bottleneck                    1.3.4\n",
      "brotlipy                      0.7.0\n",
      "cachetools                    4.2.2\n",
      "certifi                       2021.10.8\n",
      "cffi                          1.15.0\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         8.0.4\n",
      "cloudpickle                   2.0.0\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.4\n",
      "colorcet                      2.0.6\n",
      "comtypes                      1.1.10\n",
      "conda                         4.12.0\n",
      "conda-build                   3.21.8\n",
      "conda-content-trust           0+unknown\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        1.8.1\n",
      "conda-repo-cli                1.0.4\n",
      "conda-token                   0.3.0\n",
      "conda-verify                  3.4.2\n",
      "constantly                    15.1.0\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  3.4.8\n",
      "cssselect                     1.1.0\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.28\n",
      "cytoolz                       0.11.0\n",
      "daal4py                       2021.5.0\n",
      "dask                          2022.2.1\n",
      "datashader                    0.13.0\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.5.1\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "distributed                   2022.2.1\n",
      "docutils                      0.17.1\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "executing                     0.8.3\n",
      "fastjsonschema                2.15.1\n",
      "filelock                      3.6.0\n",
      "flake8                        3.9.2\n",
      "Flask                         1.1.2\n",
      "fonttools                     4.25.0\n",
      "frozenlist                    1.2.0\n",
      "fsspec                        2022.2.0\n",
      "future                        0.18.2\n",
      "gensim                        4.1.2\n",
      "glob2                         0.7\n",
      "google-api-core               1.25.1\n",
      "google-auth                   1.33.0\n",
      "google-cloud-core             1.7.1\n",
      "google-cloud-storage          1.31.0\n",
      "google-crc32c                 1.1.2\n",
      "google-resumable-media        1.3.1\n",
      "googleapis-common-protos      1.53.0\n",
      "greenlet                      1.1.1\n",
      "grpcio                        1.42.0\n",
      "h5py                          3.6.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.14.8\n",
      "huggingface-hub               0.7.0\n",
      "hvplot                        0.7.3\n",
      "hyperlink                     21.0.0\n",
      "idna                          3.3\n",
      "imagecodecs                   2021.8.26\n",
      "imageio                       2.9.0\n",
      "imagesize                     1.3.0\n",
      "importlib-metadata            4.11.3\n",
      "incremental                   21.3.0\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "intake                        0.6.5\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.9.1\n",
      "ipython                       8.2.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.6.5\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  2.0.1\n",
      "jdcal                         1.4.1\n",
      "jedi                          0.18.1\n",
      "Jinja2                        2.11.3\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.1.0\n",
      "json5                         0.9.6\n",
      "jsonschema                    4.4.0\n",
      "jupyter                       1.0.0\n",
      "jupyter-client                6.1.12\n",
      "jupyter-console               6.4.0\n",
      "jupyter-core                  4.9.2\n",
      "jupyter-server                1.13.5\n",
      "jupyterlab                    3.3.2\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab-server             2.10.3\n",
      "jupyterlab-widgets            1.0.0\n",
      "keyring                       23.4.0\n",
      "kiwisolver                    1.3.2\n",
      "lazy-object-proxy             1.6.0\n",
      "libarchive-c                  2.9\n",
      "llvmlite                      0.38.0\n",
      "locket                        0.2.1\n",
      "lxml                          4.8.0\n",
      "Markdown                      3.3.4\n",
      "MarkupSafe                    2.0.1\n",
      "matplotlib                    3.5.1\n",
      "matplotlib-inline             0.1.2\n",
      "mccabe                        0.6.1\n",
      "menuinst                      1.4.18\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.1\n",
      "mkl-random                    1.2.2\n",
      "mkl-service                   2.4.0\n",
      "mock                          4.0.3\n",
      "more-itertools                8.13.0\n",
      "mpmath                        1.2.1\n",
      "msgpack                       1.0.2\n",
      "multidict                     5.1.0\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "mypy-extensions               0.4.3\n",
      "navigator-updater             0.2.1\n",
      "nbclassic                     0.3.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.4.4\n",
      "nbformat                      5.3.0\n",
      "nest-asyncio                  1.5.5\n",
      "networkx                      2.7.1\n",
      "nltk                          3.7\n",
      "nose                          1.3.7\n",
      "notebook                      6.4.8\n",
      "numba                         0.55.1\n",
      "numexpr                       2.8.1\n",
      "numpy                         1.21.5\n",
      "numpydoc                      1.2\n",
      "olefile                       0.46\n",
      "openpyxl                      3.0.9\n",
      "packaging                     21.3\n",
      "pandas                        1.4.2\n",
      "pandocfilters                 1.5.0\n",
      "panel                         0.13.0\n",
      "param                         1.12.0\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.2.0\n",
      "pathspec                      0.7.0\n",
      "patsy                         0.5.2\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.0.1\n",
      "pip                           21.2.4\n",
      "pkginfo                       1.8.2\n",
      "plotly                        5.6.0\n",
      "pluggy                        1.0.0\n",
      "poyo                          0.5.0\n",
      "prometheus-client             0.13.1\n",
      "prompt-toolkit                3.0.20\n",
      "Protego                       0.1.16\n",
      "protobuf                      3.19.1\n",
      "psutil                        5.8.0\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "py                            1.11.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.7.0\n",
      "pycosat                       0.6.3\n",
      "pycparser                     2.21\n",
      "pyct                          0.4.6\n",
      "pycurl                        7.44.1\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.1.1\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      2.3.1\n",
      "Pygments                      2.11.2\n",
      "PyHamcrest                    2.0.2\n",
      "PyJWT                         2.1.0\n",
      "pylint                        2.9.6\n",
      "pyls-spyder                   0.4.0\n",
      "PyNaCl                        1.4.0\n",
      "pyodbc                        4.0.32\n",
      "pyOpenSSL                     21.0.0\n",
      "pyparsing                     3.0.4\n",
      "pyreadline                    2.1\n",
      "pyrsistent                    0.18.0\n",
      "PySimpleGUI                   4.60.1\n",
      "PySocks                       1.7.1\n",
      "pytest                        7.1.1\n",
      "python-dateutil               2.8.2\n",
      "python-lsp-black              1.0.0\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.2.4\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.0\n",
      "pytz                          2021.3\n",
      "pyviz-comms                   2.0.2\n",
      "PyWavelets                    1.3.0\n",
      "pywin32                       302\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.2\n",
      "PyYAML                        6.0\n",
      "pyzmq                         22.3.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.1.10\n",
      "QtAwesome                     1.0.3\n",
      "qtconsole                     5.3.0\n",
      "QtPy                          2.0.1\n",
      "queuelib                      1.5.0\n",
      "regex                         2022.3.15\n",
      "requests                      2.27.1\n",
      "requests-file                 1.5.1\n",
      "rope                          0.22.0\n",
      "rsa                           4.7.2\n",
      "Rtree                         0.9.7\n",
      "ruamel-yaml-conda             0.15.100\n",
      "s3transfer                    0.5.0\n",
      "scikit-image                  0.19.2\n",
      "scikit-learn                  1.0.2\n",
      "scikit-learn-intelex          2021.20220215.102710\n",
      "scipy                         1.7.3\n",
      "Scrapy                        2.6.1\n",
      "seaborn                       0.11.2\n",
      "Send2Trash                    1.8.0\n",
      "service-identity              18.1.0\n",
      "setuptools                    61.2.0\n",
      "sip                           4.19.13\n",
      "six                           1.16.0\n",
      "smart-open                    5.1.0\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcollections             2.1.0\n",
      "sortedcontainers              2.4.0\n",
      "soupsieve                     2.3.1\n",
      "Sphinx                        4.4.0\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder                        5.1.5\n",
      "spyder-kernels                2.1.3\n",
      "SQLAlchemy                    1.4.32\n",
      "stack-data                    0.2.0\n",
      "statsmodels                   0.13.2\n",
      "sympy                         1.10.1\n",
      "tables                        3.6.1\n",
      "tabulate                      0.8.9\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.0.1\n",
      "terminado                     0.13.1\n",
      "testpath                      0.5.0\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2021.7.2\n",
      "tinycss                       0.4\n",
      "tldextract                    3.2.0\n",
      "tokenizers                    0.12.1\n",
      "toml                          0.10.2\n",
      "tomli                         1.2.2\n",
      "toolz                         0.11.2\n",
      "torch                         1.10.1+cu113\n",
      "torchaudio                    0.10.1+cu113\n",
      "torchvision                   0.11.2+cu113\n",
      "tornado                       6.1\n",
      "tqdm                          4.64.0\n",
      "traitlets                     5.1.1\n",
      "transformers                  4.19.2\n",
      "Twisted                       22.2.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typed-ast                     1.4.3\n",
      "typing_extensions             4.1.1\n",
      "ujson                         5.1.0\n",
      "Unidecode                     1.2.0\n",
      "urllib3                       1.26.9\n",
      "w3lib                         1.21.0\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      2.0.3\n",
      "wheel                         0.37.1\n",
      "widgetsnbextension            3.5.2\n",
      "win-inet-pton                 1.1.0\n",
      "win-unicode-console           0.5\n",
      "wincertstore                  0.2\n",
      "wrapt                         1.12.1\n",
      "xarray                        0.20.1\n",
      "xlrd                          2.0.1\n",
      "XlsxWriter                    3.0.3\n",
      "xlwings                       0.24.9\n",
      "yapf                          0.31.0\n",
      "yarl                          1.6.3\n",
      "zict                          2.0.0\n",
      "zipp                          3.7.0\n",
      "zope.interface                5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e0631",
   "metadata": {},
   "source": [
    "# Assumptions Made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94713edf",
   "metadata": {},
   "source": [
    "# AntiFix\n",
    "\n",
    "### **AntiFix** is a creative software implemented in Python to generate 'Creative' solutions for Industrial Engineering.\n",
    "\n",
    "## My Definition of Creativity:\n",
    "\n",
    "According to Hoorn, he believes that Creativity is when two exemplars of two completely different categories are associated together to form a novel idea/concept.\n",
    "\n",
    "## Following the above definition of Creativity:\n",
    "\n",
    "Essentially, it replicates the creativity model: ACASIA, which was craeted by Dr Johan F Hoorn. Hence, Antifix in a sense is a Proof of Concept to show that the ACASIA model does indeed replicate 'Creativity'.\n",
    "\n",
    "**ACASIA** is an acronomym for: Association, Combination, Abstraction, Selection, Integration, Adaptation.\n",
    "\n",
    "- **Association**: \n",
    "•\tcapacity to generate semantically related features in response to a stimulus.\n",
    "- **Combination**:\n",
    "•\tConnections between associations of disparate entities are established, rooted in a matching mechanism of fuzzy feature sets and reflected in a measure of perceived similarity (and hence also dissimilarity).\n",
    "-\t**Abstraction**\n",
    "•\tMeans to bring certain phenomena on such a conceptual level that the Combination can actually take place. Essentially making the concept/theory.\n",
    "-\t**Selection**\n",
    "•\tDismissal of features that cannot be used to make the combination acceptable. This affects the level of dissimilarity.\n",
    "-\t**Integration**\n",
    "•\tThe activity to attach the features of one entity to the other.\n",
    "-\t**Adaptation**\n",
    "•\tTo change individual features so that the combination would become acceptable.\n",
    "\n",
    "# Essentially the ACASIA model works as follows:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/38829497/167597763-9e2447c6-bac6-43e2-89c8-8405642f1352.png)\n",
    "\n",
    "\n",
    "# What does AntiFix/ACASIA solve?\n",
    "\n",
    "- Both AntiFix & ACASIA is used to overcome **'Creative-Fixation**. Creative Fixation occurs when people hold onto the conventional means of 'this how it's done'. If the conventional way does not work, then they will try even harder doing the same thing over & over.\n",
    "\n",
    "- AntiFix (Implementation of ACASIA) will attempt to overcome this Creative-Fixation by simulating the process of creating novel design-ideas.\n",
    "\n",
    "# Proof-Of-Concept:\n",
    "\n",
    "- As a POC, Antifix will be specialized in attempting to solve Industrial/Engineering issues w/ Biomimicry. \n",
    "- A good example of this can be seen in the image below:\n",
    "- ![image](https://user-images.githubusercontent.com/38829497/167597517-ed1e7857-f852-4ac7-8f71-01b676496e3e.png)\n",
    "\n",
    "\n",
    "# Current Implementation:\n",
    "\n",
    "- **Implementation** will be done in Text. Meaning, AntiFix will deal with Natural Language Processing problems such as Sentimental Analysis, etc.\n",
    "- Current Working Environment is done in Jupyter Notebook & written in Python.\n",
    "- Theory-Wiese all credits go to Dr Johan F Hoorn. \n",
    "  - You can view his research into ACASIA within the supplementary folder in this repository.\n",
    "  - https://www.researchgate.net/profile/Johan-Hoorn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39992201",
   "metadata": {},
   "source": [
    "## Metaphor Production will be used to make novel Associations.\n",
    "\n",
    "\n",
    "## Theory for Determing Metaphor\n",
    "\n",
    "**Metahpors** \n",
    "\n",
    "- can be used to open up meanings that would otherwise go unnoticed. \n",
    "\n",
    "- can be used to describe first-time observations in terms familiar to the observer. By linking up with known concepts, the new entity can be better understood.\n",
    "\n",
    "-  First time encounter of a new phenomenon often stimulates metaphor-production.\n",
    "\n",
    "## Assumptions made\n",
    "\n",
    "- According to Dr Johan, sufficient if the exemplars/words evoke a number of features which are related to a degree.\n",
    "\n",
    "- The said features can have a wide description. In this paper, **the features will be a simple attribute.** The features of this paper will be retrieved from both a semantic network for experimental purposes.\n",
    "\n",
    "- However, for Proof Of Concept to show how the code will work as intended, this paper will also use pre-defined features retrieved from human assumptions.\n",
    "\n",
    "## Types of Features\n",
    "\n",
    "- **Literal Feature:** plainly descirbes an **aspect of the entity that the word refers to.** (e.g a feautre of a man is his nose.) **Feature is taken literally** in the eye of the observer.\n",
    "\n",
    "- **Figurative Feature:** refers to **something else than the plain description of things as they supposedly are.** Figurative Features are connected to almost any concept or word, whether they are part of a metaphoric construction or not. **Figurative in the sense is anything that is not a literal description** of the entity the word refers to. (e.g 'Rose' not only activates 'flower' but also 'passion'.)\n",
    "\n",
    "## Three Main Arguments\n",
    "\n",
    "- **Metaphor**: Metaphors state that **A** is **B** or **Noun1 is Noun2** under the condition that **nouns are out-of-category.\n",
    "    - Out-Of-Category, Large Intersection.\n",
    "\n",
    "- **Literal-Statements**: Statement is in Category.\n",
    "    - In-category, Large Intersection.\n",
    "    \n",
    "- **Anomalies**: Statement is out-of-category but also nonseniscal.\n",
    "    - Out of Category, Small Intersection\n",
    "\n",
    "## How to Identify a Metaphor\n",
    "\n",
    "- **Restriction & Assumption**: \n",
    "    - Metaphor will be treated as **Noun1 is Noun2** (According to Johan, more complex metaphors are built from such basic comparisons)\n",
    "    - Whether comparisons are treated as literal, metaphoric or amaomalous, number of literal & figurative features are activated when people encounter a noun-noun construction.\n",
    "    \n",
    "- **Literal Context**: Figurative Features suppressed more compared to Literal Features. **Figuaritive Context** is the opposite.\n",
    "\n",
    "- **Seen as a race between Literal & Figurative information sources** but **division in a literal and figurative set for each word is not unchallenged.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde98ea",
   "metadata": {},
   "source": [
    "## Additional Theory\n",
    "\n",
    "- Metaphor will take into account all possible combinations between the undifferentiated features of Noun1 & Noun2:\n",
    "    \n",
    "    -**Noun1 literal - Noun2 literal**\n",
    "    \n",
    "    -**Noun1 Figurative - Noun2 Figurative**\n",
    "    \n",
    "    -**Noun1 Literal - Noun2 Figurative**\n",
    "    \n",
    "    -**Noun1 Figurative - Noun2 Literal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371500c",
   "metadata": {},
   "source": [
    "# Experiment 1: Create Associations from Pre-Defined Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fa024",
   "metadata": {},
   "source": [
    "# Take User-Defined Critiera for determining Associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d5b2089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaphor Threshold:0.7\n",
      "Literal Threshold:0.9\n"
     ]
    }
   ],
   "source": [
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class MetaphorOutofRange(Error):\n",
    "    \"\"\"Raised when the Metaphor Threshold is not between 0 or 1\"\"\"\n",
    "    pass\n",
    "\n",
    "class LiteralOutofRange(Error):\n",
    "    \"\"\"Raised when the Literal Threshold is not between 0 or 1 or is less than the Metaphor Threshold\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Metaphor Threshold\n",
    "while True:\n",
    "    try:\n",
    "        metaphor_threshold = float(input(\"Metaphor Threshold:\"))\n",
    "        \n",
    "        if(metaphor_threshold < 0.0 or metaphor_threshold > 1.0):\n",
    "            raise MetaphorOutofRange\n",
    "        break\n",
    "        \n",
    "    except MetaphorOutofRange:\n",
    "        print('\\nThreshold needs to be between 0 or 1 to determine whether it is a metaphor.')\n",
    "        \n",
    "\n",
    "# Literal Threshold\n",
    "while True:\n",
    "    try:\n",
    "        literal_threshold = float(input(\"Literal Threshold:\"))\n",
    "        \n",
    "        if(literal_threshold < 0.0 or literal_threshold > 1.0 or literal_threshold < metaphor_threshold):\n",
    "            raise LiteralOutofRange\n",
    "        break\n",
    "        \n",
    "    except LiteralOutofRange:\n",
    "        print('\\nThreshold needs to be between 0 or 1 to & must be greater than the Metaphor_Threshold.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff800cfe",
   "metadata": {},
   "source": [
    "# Take Pre-Defined Dataset for Biomimicry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb48cafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@plant [catg] ', 'flower l ', 'leafs l ', 'stem l ', 'petals l ', 'thorns l ', 'fruit l ', 'water l ', 'photosynthesis l ', 'forest l ', 'trees l ', 'soil l ', 'natural-life f ', '# ', '@animal [catg] ', 'mammal l +land l +lactation l ', 'bird l +air l +wings l +eggs l +hollow-bones l ', 'insect l +air l +wings l +exoskeleton l ', 'fish l +water l +gills l +scales l ', 'reptile l +land l +eggs l ', 'amphibious l +water l +land l ambiguous f ', 'eyes l ', 'ears l ', 'legs l ', 'nose l ', 'tail l ', 'metabolism l ', 'ferocious f ', '# ', '@amphibian [catg] ', 'land l', 'water l', 'warm-environment l', 'amphibious l +water l +land l ambiguous f ', 'smooth l', '# ', '@monkey [exm>animal] ', 'mammal l ', 'eyes l ', 'ears l ', 'legs l ', 'nose l ', 'tree l +climbing l ', 'long-tail l ', 'metabolism l ', 'fool f ', '# ', '@gecko [exm>amphibian] ', 'sticky l', 'firm l', 'grip l', '', '# ', '', ' ', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "with open (\"biomimicry_data.txt\", \"r\") as myfile:\n",
    "    data = myfile.read().splitlines()\n",
    "    \n",
    "myfile.close()\n",
    "\n",
    "# Remove all empty-space elements\n",
    "for i in data: \n",
    "    if(i == ''): \n",
    "        data.remove('')\n",
    "        \n",
    "for i in data:\n",
    "    if(i == ' '):\n",
    "        data.remove(' ')\n",
    "        \n",
    "# Remove Trailing Space and Leading Space of each element\n",
    "for i in data:\n",
    "    i.strip()\n",
    "   \n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a696908",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_data_list = []\n",
    "bio_categories = []\n",
    "bio_exemplars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c8c7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import locate\n",
    "# Retrieve Categories\n",
    "\n",
    "indexes = list(locate(data, lambda x: x == '# '))\n",
    "\n",
    "offset = 0\n",
    "for i in indexes:\n",
    "    bio_data_list.append(data[offset:i+1])\n",
    "    offset = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ce5397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in bio_data_list:\n",
    "    \n",
    "    if('[catg]' in item[0]):\n",
    "        category = item[0].replace('@', '')\n",
    "        \n",
    "        cat = category.split(' ')\n",
    "        \n",
    "        bio_categories.append(cat[0])\n",
    "        \n",
    "    elif('[catg]' in item[1]):\n",
    "        category = item[1].replace('@', '')\n",
    "        \n",
    "        cat = category.split(' ')\n",
    "        \n",
    "        bio_categories.append(cat[0])\n",
    "        \n",
    "    if('[exm>' in item[0] or '[exm>' in item[1]):\n",
    "        bio_exemplars.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d185c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@monkey [exm>animal] ', 'mammal l ', 'eyes l ', 'ears l ', 'legs l ', 'nose l ', 'tree l +climbing l ', 'long-tail l ', 'metabolism l ', 'fool f ', '# ']\n",
      "['@gecko [exm>amphibian] ', 'sticky l', 'firm l', 'grip l', '', '# ']\n"
     ]
    }
   ],
   "source": [
    "for exemplar in bio_exemplars:\n",
    "    if(' ' in exemplar):\n",
    "        exemplar.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09115171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plant', 'animal', 'amphibian']\n",
      "[['@monkey [exm>animal] ', 'mammal l ', 'eyes l ', 'ears l ', 'legs l ', 'nose l ', 'tree l +climbing l ', 'long-tail l ', 'metabolism l ', 'fool f ', '# '], ['@gecko [exm>amphibian] ', 'sticky l', 'firm l', 'grip l', '', '# ']]\n"
     ]
    }
   ],
   "source": [
    "print(bio_categories)\n",
    "print(bio_exemplars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c88e2",
   "metadata": {},
   "source": [
    "# Take Pre-Defined Dataset for Engineering Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a2e1da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@tool [catg] ', 'easy l', 'flexible f', 'modify l', 'measure l', 'Holding l', 'cutting l', 'driving l', 'boring l +electrical l +miscellaneous l', '# ', '@glove [exm>tool] ', 'grip l', 'durable l', 'waterproof l', 'protection l', 'warm l', 'flexible l', '# ', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "with open (\"engineering_data.txt\", \"r\") as myfile:\n",
    "    data = myfile.read().splitlines()\n",
    "    \n",
    "myfile.close()\n",
    "\n",
    "# Remove all empty-space elements\n",
    "for i in data: \n",
    "    if(i == ''): \n",
    "        data.remove('')\n",
    "        \n",
    "for i in data:\n",
    "    if(i == ' '):\n",
    "        data.remove(' ')\n",
    "        \n",
    "# Remove Trailing Space and Leading Space of each element\n",
    "for i in data:\n",
    "    i.strip()\n",
    "   \n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f88aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data_list = []\n",
    "eng_categories = []\n",
    "eng_exemplars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06980a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import locate\n",
    "# Retrieve Categories\n",
    "\n",
    "indexes = list(locate(data, lambda x: x == '# '))\n",
    "\n",
    "offset = 0\n",
    "for i in indexes:\n",
    "    eng_data_list.append(data[offset:i+1])\n",
    "    offset = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "341484af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in eng_data_list:\n",
    "    \n",
    "    if('[catg]' in item[0]):\n",
    "        category = item[0].replace('@', '')\n",
    "        \n",
    "        cat = category.split(' ')\n",
    "        \n",
    "        eng_categories.append(cat[0])\n",
    "        \n",
    "    elif('[catg]' in item[1]):\n",
    "        category = item[1].replace('@', '')\n",
    "        \n",
    "        cat = category.split(' ')\n",
    "        \n",
    "        eng_categories.append(cat[0])\n",
    "        \n",
    "    if('[exm>' in item[0] or '[exm>' in item[1]):\n",
    "        eng_exemplars.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2fba97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exemplar in eng_exemplars:\n",
    "    if(' ' in exemplar):\n",
    "        exemplar.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97ec90ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tool']\n",
      "[['@glove [exm>tool] ', 'grip l', 'durable l', 'waterproof l', 'protection l', 'warm l', 'flexible l', '# ']]\n"
     ]
    }
   ],
   "source": [
    "print(eng_categories)\n",
    "print(eng_exemplars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb64a6d",
   "metadata": {},
   "source": [
    "# HeapSort to sort the Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "204ca47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program for implementation of heap Sort\n",
    " \n",
    "# To heapify subtree rooted at index i.\n",
    "# n is size of heap\n",
    " \n",
    " \n",
    "def heapifyCosine(arr, n, i):\n",
    "    largest = i  # Initialize largest as root\n",
    "    l = 2 * i + 1     # left = 2*i + 1\n",
    "    r = 2 * i + 2     # right = 2*i + 2\n",
    " \n",
    "    # See if left child of root exists and is\n",
    "    # greater than root\n",
    "    if l < n and arr[largest]['cosine_score'] < arr[l]['cosine_score']:\n",
    "        largest = l\n",
    " \n",
    "    # See if right child of root exists and is\n",
    "    # greater than root\n",
    "    if r < n and arr[largest]['cosine_score'] < arr[r]['cosine_score']:\n",
    "        largest = r\n",
    " \n",
    "    # Change root, if needed\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]  # swap\n",
    " \n",
    "        # Heapify the root.\n",
    "        heapifyCosine(arr, n, largest)\n",
    " \n",
    "# The main function to sort an array of given size\n",
    " \n",
    " \n",
    "def heapSortCosine(arr):\n",
    "    n = len(arr)\n",
    " \n",
    "    # Build a maxheap.\n",
    "    for i in range(n//2 - 1, -1, -1):\n",
    "        heapifyCosine(arr, n, i)\n",
    " \n",
    "    # One by one extract elements\n",
    "    for i in range(n-1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]  # swap\n",
    "        heapifyCosine(arr, i, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54fd68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapifyJaccard(arr, n, i):\n",
    "    largest = i  # Initialize largest as root\n",
    "    l = 2 * i + 1     # left = 2*i + 1\n",
    "    r = 2 * i + 2     # right = 2*i + 2\n",
    " \n",
    "    # See if left child of root exists and is\n",
    "    # greater than root\n",
    "    if l < n and arr[largest]['jaccard_score'] < arr[l]['jaccard_score']:\n",
    "        largest = l\n",
    " \n",
    "    # See if right child of root exists and is\n",
    "    # greater than root\n",
    "    if r < n and arr[largest]['jaccard_score'] < arr[r]['jaccard_score']:\n",
    "        largest = r\n",
    " \n",
    "    # Change root, if needed\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]  # swap\n",
    " \n",
    "        # Heapify the root.\n",
    "        heapifyJaccard(arr, n, largest)\n",
    " \n",
    "# The main function to sort an array of given size\n",
    " \n",
    " \n",
    "def heapSortJaccard(arr):\n",
    "    n = len(arr)\n",
    " \n",
    "    # Build a maxheap.\n",
    "    for i in range(n//2 - 1, -1, -1):\n",
    "        heapifyJaccard(arr, n, i)\n",
    " \n",
    "    # One by one extract elements\n",
    "    for i in range(n-1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]  # swap\n",
    "        heapifyJaccard(arr, i, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651a042",
   "metadata": {},
   "source": [
    "# Three Conditions for Each Result\n",
    "\n",
    "- Metaphors = Out Of Category Associations, Large Intersection of Features\n",
    "\n",
    "- Anomaly = Out of Category Associations, Small Intersection of Features\n",
    "\n",
    "- Literal = In Category Associations, Large Intersection of Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d5ba4",
   "metadata": {},
   "source": [
    "# Assumptions Made:\n",
    "\n",
    "- Set-Theory/Fuzzy-Matching is done between Nouns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec5535",
   "metadata": {},
   "source": [
    "# To determine Metaphor, Anomaly, Literal:\n",
    "\n",
    "- significant **difference between literal statements and metaphors** was in the size of the **intersection between features that were literal for the one noun and figurative** for the other **(Noun1 literal-Noun2 figurative and Noun1 figurative - Noun2 literal)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e04e1",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6afd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def tfid_vectorize_cosine(featurelist1, featurelist2):\n",
    "    \n",
    "\n",
    "    Tfidf_vect = TfidfVectorizer()\n",
    "    \n",
    "    # Stringify List to String to vectorize\n",
    "    emotion_features_str = ' '.join(featurelist1)\n",
    "    exemplar_features_str = ' '.join(featurelist2)\n",
    "    \n",
    "    # Convert to list to vectorize\n",
    "    data = [emotion_features_str, exemplar_features_str]\n",
    "    \n",
    "    vector_matrix = Tfidf_vect.fit_transform(data)\n",
    "    \n",
    "     # Convert Vector Matrix to arrayc\n",
    "    vector_matrix.toarray()\n",
    "    \n",
    "    tokens = Tfidf_vect.get_feature_names()\n",
    "    \n",
    "\n",
    "    # Generate Cosine Similarity score\n",
    "    cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "    \n",
    "  \n",
    "    \n",
    "    return [cosine_similarity_matrix, tokens, vector_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63492a7",
   "metadata": {},
   "source": [
    "# Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d8eae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(doc1, doc2): \n",
    "    \n",
    "    # List the unique words in a document\n",
    "    words_doc1 = set(doc1) \n",
    "    words_doc2 = set(doc2)\n",
    "    \n",
    "    # Find the intersection of words list of doc1 & doc2\n",
    "    intersection = words_doc1.intersection(words_doc2)\n",
    "\n",
    "    # Find the union of words list of doc1 & doc2\n",
    "    union = words_doc1.union(words_doc2)\n",
    "        \n",
    "    # Calculate Jaccard similarity score \n",
    "    # using length of intersection set divided by length of union set\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea10eb",
   "metadata": {},
   "source": [
    "# Fuzzy Matching of Matching Features (Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3658d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'exemplar1': '@monkey [exm>animal] ', 'exemplar2': '@glove [exm>tool] ', 'cosine_score': 0.04723062045463176, 'jaccard_score': 0.0}, {'exemplar1': '@gecko [exm>amphibian] ', 'exemplar2': '@glove [exm>tool] ', 'cosine_score': 0.15976420924144444, 'jaccard_score': 0.09090909090909091}]\n"
     ]
    }
   ],
   "source": [
    "# Fuzzy Matching\n",
    "associations = []\n",
    "\n",
    "for exemplar1 in bio_exemplars:\n",
    "\n",
    "    noun1List = []\n",
    "    \n",
    "    for i in range(0, len(exemplar1)):\n",
    "        \n",
    "        if('#' in exemplar1[i]):\n",
    "            continue\n",
    "        \n",
    "        noun1List.append(exemplar1[i])\n",
    "    \n",
    "    for exemplar2 in eng_exemplars:\n",
    "        \n",
    "        noun2List = []\n",
    "        \n",
    "        for j in range(0, len(exemplar2)):\n",
    "            \n",
    "            if('#' in exemplar2[j]):\n",
    "                continue\n",
    "            \n",
    "            noun2List.append(exemplar2[j])\n",
    "                \n",
    "            \n",
    "        cosine_similarity_results = tfid_vectorize_cosine(noun1List, noun2List)\n",
    "        \n",
    "        cosine_similarity_score = cosine_similarity_results[0][0][1]\n",
    "        cosine_features = cosine_similarity_results[1]\n",
    "        cos_matrix = cosine_similarity_results[2]\n",
    "        \n",
    "        jaccard_similarity_score = jaccard_similarity(noun1List, noun2List)\n",
    "        \n",
    "        \n",
    "        \n",
    "        associations.append(\n",
    "            dict( \n",
    "                exemplar1 = noun1List[0], \n",
    "                exemplar2 = noun2List[0], \n",
    "                cosine_score = cosine_similarity_score, \n",
    "                jaccard_score = jaccard_similarity_score, \n",
    "                \n",
    "            ))\n",
    "\n",
    "\n",
    "print(associations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories = []\n",
    "\n",
    "categories = bio_categories + eng_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f6d7f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaphor-Threshold: 0.7 Literal-Threshold 0.9 \n",
      "\n",
      "2\n",
      "@monkey [exm>animal]  means monkey is an exemplar in animal\n",
      "@glove [exm>tool]  means glove is an exemplar in tool\n",
      "@monkey [exm>animal]  @glove [exm>tool]  is Anomaly\n",
      "0.04723062045463176\n",
      "\n",
      "\n",
      "@gecko [exm>amphibian]  means gecko is an exemplar in amphibian\n",
      "@glove [exm>tool]  means glove is an exemplar in tool\n",
      "@gecko [exm>amphibian]  @glove [exm>tool]  is Anomaly\n",
      "0.15976420924144444\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for Cosine Similarity\n",
    "heapSortCosine(associations)\n",
    "\n",
    "print('Metaphor-Threshold:', metaphor_threshold, 'Literal-Threshold', literal_threshold, '\\n')\n",
    "\n",
    "print(len(associations))\n",
    "\n",
    "for association in associations:\n",
    "    \n",
    "    exemplar1_noun = association['exemplar1'].split(' ')[0]\n",
    "    exemplar2_noun = association['exemplar2'].split(' ')[0]\n",
    "    \n",
    "    exemplar1_noun = exemplar1_noun.replace('@', '')\n",
    "    exemplar2_noun = exemplar2_noun.replace('@', '')\n",
    "    \n",
    "    for category in categories:\n",
    "        \n",
    "        if(category in association['exemplar1']):\n",
    "           \n",
    "            noun1Category = category\n",
    "            \n",
    "        if(category in association['exemplar2']):\n",
    "            \n",
    "            noun2Category = category\n",
    "    \n",
    "    # Print out the Category Associations\n",
    "    print(association['exemplar1'], 'means', exemplar1_noun,'is an exemplar in', noun1Category)\n",
    "    print(association['exemplar2'], 'means', exemplar2_noun,'is an exemplar in', noun2Category)\n",
    "    \n",
    "    # Literal when the Similarity of the Association is beyond the Metaphor Threshold\n",
    "    if(association['cosine_score'] > literal_threshold):\n",
    "        print(association['exemplar1'], association['exemplar2'],'is Literal')\n",
    "    \n",
    "    # Anomaly is when the Simiilarity of the Association is below the Metaphor Threshold\n",
    "    elif(association['cosine_score'] < metaphor_threshold):\n",
    "         print(association['exemplar1'], association['exemplar2'],'is Anomaly')\n",
    "    \n",
    "    # Metaphor must be Out of Category Associations! Meets the Metaphor Threshold\n",
    "    elif(noun1Category != noun2Category):\n",
    "        print(association['exemplar1'], association['exemplar2'],'is Metaphor')\n",
    "    \n",
    "    # Meets the Metaphor Threshold but the Association is In-Category..\n",
    "    else:\n",
    "        print(association['exemplar1'], association['exemplar2'],'is Anomaly')\n",
    "        \n",
    "    print(association['cosine_score'])\n",
    "    \n",
    "    if(association['cosine_score'] >= 0.99):\n",
    "        print('Tautology.')\n",
    "        \n",
    "    #print(association['matching_features'])\n",
    "    \n",
    "   \n",
    "    print('\\n')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca365060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaphor-Threshold: 0.7 Literal-Threshold 0.9 \n",
      "\n",
      "@monkey [exm>animal]  means monkey is an exemplar in animal\n",
      "@glove [exm>tool]  means glove is an exemplar in tool\n",
      "@monkey [exm>animal]  @glove [exm>tool]  is Anomaly\n",
      "0.0\n",
      "\n",
      "\n",
      "@gecko [exm>amphibian]  means gecko is an exemplar in amphibian\n",
      "@glove [exm>tool]  means glove is an exemplar in tool\n",
      "@gecko [exm>amphibian]  @glove [exm>tool]  is Anomaly\n",
      "0.09090909090909091\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for Jaccard Similarity\n",
    "heapSortJaccard(associations)\n",
    "\n",
    "print('Metaphor-Threshold:', metaphor_threshold, 'Literal-Threshold', literal_threshold, '\\n')\n",
    "\n",
    "for association in associations:\n",
    "    \n",
    "\n",
    "    exemplar1_noun = association['exemplar1'].split(' ')[0]\n",
    "    exemplar2_noun = association['exemplar2'].split(' ')[0]\n",
    "    \n",
    "    exemplar1_noun = exemplar1_noun.replace('@', '')\n",
    "    exemplar2_noun = exemplar2_noun.replace('@', '')\n",
    "    \n",
    "    for category in categories:\n",
    "        \n",
    "        if(category in association['exemplar1']):\n",
    "           \n",
    "            noun1Category = category\n",
    "            \n",
    "        if(category in association['exemplar2']):\n",
    "            \n",
    "            noun2Category = category\n",
    "    \n",
    "    # Print out the Category Associations\n",
    "    print(association['exemplar1'], 'means', exemplar1_noun,'is an exemplar in', noun1Category)\n",
    "    print(association['exemplar2'], 'means', exemplar2_noun,'is an exemplar in', noun2Category)\n",
    "    \n",
    "    # Literal when the Similarity of the Association is beyond the Metaphor Threshold\n",
    "    if(association['jaccard_score'] > literal_threshold):\n",
    "        print(association['exemplar1'], association['exemplar2'], 'is Literal')\n",
    "    \n",
    "    # Anomaly is when the Simiilarity of the Association is below the Metaphor Threshold\n",
    "    elif(association['jaccard_score'] < metaphor_threshold):\n",
    "         print(association['exemplar1'], association['exemplar2'], 'is Anomaly')\n",
    "    \n",
    "    # Metaphor must be Out of Category Associations! Meets the Metaphor Threshold\n",
    "    elif(noun1Category != noun2Category):\n",
    "        print(association['exemplar1'], association['exemplar2'], 'is Metaphor')\n",
    "    \n",
    "    # Meets the Metaphor Threshold but the Association is In-Category..\n",
    "    else:\n",
    "        print(association['exemplar1'], association['exemplar2'], 'is Literal')\n",
    "        \n",
    "    print(association['jaccard_score'])\n",
    "    print('\\n')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb28c24",
   "metadata": {},
   "source": [
    "# Experiment 2: Emotion-Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81dd7e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Input value of Criterion Q:30\n",
      "Criterion Q: 30\n"
     ]
    }
   ],
   "source": [
    "# Criterion Q determines the number of features as a threshold to compare to.\n",
    "# Exemplar to compare must also be a noun!\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        criterionQ = int(input(\"Please Input value of Criterion Q:\"))\n",
    "        break\n",
    "        \n",
    "    except ValueError:\n",
    "        print('Criterion Q needs to be an Integer!')\n",
    "\n",
    "print(\"Criterion Q:\", criterionQ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217026c6",
   "metadata": {},
   "source": [
    "# User Input to determine 2nd Category (must be a noun) to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "203435bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines Second Exemplar to associate our concepts with.\n",
    "\n",
    "# Definition of a Noun: A person, place, thing or \"idea\"\n",
    "\n",
    "# Nouns are basically used to convey words that could potentially represent concepts.\n",
    "# Thus, nouns would be suitable as our exemplar to compare to.\n",
    "\n",
    "\n",
    "# Import from NLTK corpus that has Wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "# retrieve nouns from Wordnet Corpus\n",
    "nouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "173373e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a Category Noun to compare to:weather\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "   \n",
    "        exemplar = input(\"Please input a Category Noun to compare to:\")\n",
    "        if(exemplar in nouns):\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            print(\"exemplar needs to be a noun!\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f3bc4",
   "metadata": {},
   "source": [
    "# Necessary libraries for BERT & NLP Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f3f0dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "# Import Trnasformer model to be used in our BERT model\n",
    "# AutoTokenizer tokenizes the string..\n",
    "# AutoModelForSequenceClassification loads in the architecture from Transformers to load our NLP model.\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Argmax function from Torch to filter out the highest-sequence result..\n",
    "import torch\n",
    "\n",
    "# Natural Language Processing Tools\n",
    "import nltk\n",
    "\n",
    "# requests used to grab data from Yelp\n",
    "import requests\n",
    "# BeautifulSoup allows us to traverse and scrape data from AskNature\n",
    "from bs4 import BeautifulSoup\n",
    "# Regex functions\n",
    "import re\n",
    "\n",
    "import string\n",
    "\n",
    "# Downloading stopwords for English from NLTK \n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords, words\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "print(len(stopWords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef99398",
   "metadata": {},
   "source": [
    "# Text-Cleaning (Data-Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "465532ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://studymachinelearning.com/text-data-cleaning-preprocessing/\n",
    "# Handles Emojis\n",
    "emojis = \"🍕🐵😑😢🐶️😜😎👊🤪😁😍💖💵👎😀😂🔥⭐🤯😄🤪🏻💥😋👏😱🚌ᴵ͞🌟😊😳😧🍕🙀😐😕👍😮😃😘💩💯⛽🚄😖🏼🚲😟😈💪🙏🎯🌹😇💔😡👌🙄😠😉😤⛺🙂😏🍾🎉😞🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪🐰🐇🐱🙆😨🙃💕💗💚🙈😴🏿🤗🇺🇸⤵🏆🎃😩👮💙🐾🐕😆🌠🐟💫💰💎🖐🙅⛲🍰🤐👆🙌💛🙁👀🙊🙉🚬🤓😵😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓😣⏺😌🤑🌏😯😲💞🚓🔔📚🏀👐💤🍇🏡❔⁉👠》🇹🇼🌸🌞🎲😛💋💀🎄💜🤢َِ🗑💃📣👿༼つ༽😰🤣🐝🎅🍺🎵🌎͟🤡🤥😬🤧🚀🤴😝💨🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦🍀😫🤤🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪🇫🇷🇩🇪😷🇨🇦🌐📺🐋💘💓💐🌋🌄🌅👺🐷🚶🤘ͦ💸👂👃🎫🚢🚂🏃👽😙🎾👹⎌🏒⛸🏄🐀🚑🤷🤙🐒🐈ﷻ🦄🚗🐳👇⛷👋🦊🐽🎻🎹⛓🏹🍷🦆♾🎸🤕🤒⛑🎁🏝🦁🙋😶🔫👁💲🗯👑🚿💡😦🏐🇰🇵👾🐄🎈🔨🐎🤞🐸💟🎰🌝🛳🍭👣🏉💭🎥🐴👨🤳🦍🍩😗🏂👳🍗🕉🐲🍒🐑⏰💊🌤🍊🔹🤚🍎𝑷🐂💅💢💒🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻🤖🎎😼🕷👼📉🍟🍦🌈🔭《🐊🐍🐦🐡💳ἱ🙇🥜🔼\"\n",
    "\n",
    "def remove_emojis(text):\n",
    "    for emoji in emojis:\n",
    "        text = text.replace(emoji, '')\n",
    "    return text\n",
    "\n",
    "# Handles URLS\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text)    \n",
    "    return text\n",
    "\n",
    "# Handles Contractions\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "# Handles Punctuations\n",
    "regular_punct = list(string.punctuation)\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "\n",
    "all_punct = list(set(regular_punct + extra_punct))\n",
    "def spacing_punctuation(text):\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035489e",
   "metadata": {},
   "source": [
    "# Enquire user to input any sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d346d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence, and I will determine how you are feeling:I feel like shit\n",
      "User-Input: I feel like shit\n"
     ]
    }
   ],
   "source": [
    "# User Input\n",
    "token_sentence = input(\"Enter a sentence, and I will determine how you are feeling:\")\n",
    "\n",
    "print(\"User-Input:\", token_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acccdba6",
   "metadata": {},
   "source": [
    "# Preprocess User Input before passing to the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5745a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "feel\n",
      "like\n",
      "shit\n",
      "['I', 'feel', 'like', 'shit']\n"
     ]
    }
   ],
   "source": [
    "# Cleans the Text\n",
    "\n",
    "token_list = []\n",
    "token_sentence = remove_emojis(token_sentence)\n",
    "token_sentence = remove_url(token_sentence)\n",
    "token_sentence = clean_contractions(token_sentence, contraction_mapping)\n",
    "token_sentence = spacing_punctuation(token_sentence)\n",
    "\n",
    "words = token_sentence.split()\n",
    "\n",
    "# Remove Stop Words from Tokenized List (removing words that don't add context)\n",
    "for word in words:\n",
    "    if word not in stopWords:\n",
    "        print(word)\n",
    "        token_list.append(word)\n",
    "\n",
    "print(token_list)\n",
    "\n",
    "token_sentence = \" \".join(str(x) for x in token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78792eeb",
   "metadata": {},
   "source": [
    "# Pass the User-Input Sentence through the model # bhadresh-savani/distilbert-base-uncased-emotion to detect overall emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95f217ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bec2e2eae645debbfcb70ebe377032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f539c0155c3f49ada55bfccdc001406c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4840e901551b41469f035782b6b36f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dccfa8acdf4eaa87329a3385a5f1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec5f8827c1749cc82e2660ec355d1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'sadness', 'score': 0.9045525789260864}, {'label': 'joy', 'score': 0.07274568825960159}, {'label': 'love', 'score': 0.0024434062652289867}, {'label': 'anger', 'score': 0.016538679599761963}, {'label': 'fear', 'score': 0.0029163563158363104}, {'label': 'surprise', 'score': 0.0008031809702515602}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n",
    "prediction = classifier(token_sentence, )\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62560f55",
   "metadata": {},
   "source": [
    "# Retrieve Emotion with highest score. Retrieved Emotion will be treated as the First Category of the association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42a3ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'sadness', 'score': 0.9045525789260864}\n"
     ]
    }
   ],
   "source": [
    "emotions = prediction[0]\n",
    "predicted_emotion = {'label': '', 'score': 0}\n",
    "\n",
    "for item in emotions:\n",
    "    \n",
    "    if  predicted_emotion['score'] < item['score']:\n",
    "        predicted_emotion = item\n",
    "\n",
    "print(predicted_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c95a1b",
   "metadata": {},
   "source": [
    "# Link to Semantic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd49ffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.1.1-cp39-cp39-win_amd64.whl (365 kB)\n",
      "Installing collected packages: pymongo\n",
      "Successfully installed pymongo-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "320ee9b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "localhost:27017: [WinError 10061] No connection could be made because the target machine actively refused it, Timeout: 30s, Topology Description: <TopologyDescription id: 629deb1b0a0d1b6fdb39386f, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [WinError 10061] No connection could be made because the target machine actively refused it')>]>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoClient\n\u001b[0;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m MongoClient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmongodb://localhost:27017\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_database_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\pymongo\\mongo_client.py:1786\u001b[0m, in \u001b[0;36mMongoClient.list_database_names\u001b[1;34m(self, session, comment)\u001b[0m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_database_names\u001b[39m(\n\u001b[0;32m   1769\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1770\u001b[0m     session: Optional[client_session\u001b[38;5;241m.\u001b[39mClientSession] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1771\u001b[0m     comment: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1772\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;124;03m\"\"\"Get a list of the names of all databases on the connected server.\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m    :Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 3.6\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_databases\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnameOnly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\pymongo\\mongo_client.py:1759\u001b[0m, in \u001b[0;36mMongoClient.list_databases\u001b[1;34m(self, session, comment, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m     cmd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m comment\n\u001b[0;32m   1758\u001b[0m admin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_database_default_options(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1759\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43madmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_read_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;66;03m# listDatabases doesn't return a cursor (yet). Fake one.\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m cursor \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirstBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabases\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madmin.$cmd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1765\u001b[0m }\n",
      "File \u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\pymongo\\database.py:765\u001b[0m, in \u001b[0;36mDatabase._retryable_read_command\u001b[1;34m(self, command, value, check, allowable_errors, read_preference, codec_options, session, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cmd\u001b[39m(session, server, sock_info, read_preference):\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command(\n\u001b[0;32m    754\u001b[0m         sock_info,\n\u001b[0;32m    755\u001b[0m         command,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    763\u001b[0m     )\n\u001b[1;32m--> 765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\pymongo\\mongo_client.py:1364\u001b[0m, in \u001b[0;36mMongoClient._retryable_read\u001b[1;34m(self, func, read_pref, session, address, retryable)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1363\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1364\u001b[0m         server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket_from_server(read_pref, server, session) \u001b[38;5;28;01mas\u001b[39;00m (sock_info, read_pref):\n\u001b[0;32m   1366\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m retrying \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retryable:\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;66;03m# A retry is not possible because this server does\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;66;03m# not support retryable reads, raise the last error.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\pymongo\\mongo_client.py:1196\u001b[0m, in \u001b[0;36mMongoClient._select_server\u001b[1;34m(self, server_selector, session, address)\u001b[0m\n\u001b[0;32m   1194\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m AutoReconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m no longer available\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m address)\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1196\u001b[0m         server \u001b[38;5;241m=\u001b[39m \u001b[43mtopology\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_selector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m server\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PyMongoError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1199\u001b[0m     \u001b[38;5;66;03m# Server selection errors in a transaction are transient.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\pymongo\\topology.py:251\u001b[0m, in \u001b[0;36mTopology.select_server\u001b[1;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_server\u001b[39m(\u001b[38;5;28mself\u001b[39m, selector, server_selection_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;124;03m\"\"\"Like select_servers, but choose a random server if several match.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m     servers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_servers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(servers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m servers[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\pymongo\\topology.py:212\u001b[0m, in \u001b[0;36mTopology.select_servers\u001b[1;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[0;32m    209\u001b[0m     server_timeout \u001b[38;5;241m=\u001b[39m server_selection_timeout\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 212\u001b[0m     server_descriptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_servers_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_server_by_address(sd\u001b[38;5;241m.\u001b[39maddress) \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m server_descriptions]\n",
      "File \u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\pymongo\\topology.py:227\u001b[0m, in \u001b[0;36mTopology._select_servers_loop\u001b[1;34m(self, selector, timeout, address)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m server_descriptions:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# No suitable servers.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m now \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServerSelectionTimeoutError(\n\u001b[0;32m    228\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, Timeout: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124ms, Topology Description: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message(selector), timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_opened()\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_check_all()\n",
      "\u001b[1;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [WinError 10061] No connection could be made because the target machine actively refused it, Timeout: 30s, Topology Description: <TopologyDescription id: 629deb1b0a0d1b6fdb39386f, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [WinError 10061] No connection could be made because the target machine actively refused it')>]>"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f5727",
   "metadata": {},
   "source": [
    "# Query Conceptnet to retrieve exemplars/nouns of retrieved Emotion Category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ff81b",
   "metadata": {},
   "source": [
    "### Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #Regular Expression\n",
    "\n",
    "conceptnetDB = client[\"conceptnet\"] #ConceptNet DB\n",
    "edges = conceptnetDB[\"edges\"] # RELATIONSHIPS\n",
    "nodes = conceptnetDB[\"nodes\"] # EDGES\n",
    "query_format = \"/c/en/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691dacc",
   "metadata": {},
   "source": [
    "### Retrieve Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word = predicted_emotion['label']\n",
    "print(search_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c94ca0",
   "metadata": {},
   "source": [
    "### Define Dictionary to hold Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotionfeatures = dict(emotion=search_word, destination_features=[], source_features=[])\n",
    "print(emotionfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9bef7",
   "metadata": {},
   "source": [
    "### Query ConceptNet to retrieve Features of retrieved emotion (1st Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db71b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Pattern to only retrieve English Elements\n",
    "filterStr = '/c/en/'\n",
    "\n",
    "# Create the query with proper format w/ the Negative_Feature retrieved.\n",
    "query = query_format + search_word\n",
    "print('\\n',query)\n",
    "    \n",
    "# Pre-Processing below does Stemming for ConceptNet. Basically if the word doesn't exist, remove a character.\n",
    "cursor = nodes.find({\"_id\": query})\n",
    "\n",
    "if(len(cursor[0]) > 0):\n",
    "    \n",
    "    # In the case there is no destination node, keep deleting characters and search again until there is a destination node.\n",
    "    while('isd' not in cursor[0]):\n",
    "        query = query[:-1]\n",
    "        cursor = nodes.find({\"_id\": query})\n",
    "        if(len(query) == 0):\n",
    "            break\n",
    "\n",
    "    print(cursor)\n",
    "\n",
    "    # compile regular expression to match any query with '/c/en' when querying MongoDB\n",
    "    r = re.compile(\".*/c/en/\")\n",
    "\n",
    "    # Retrieve the relations & features.\n",
    "    for doc in cursor: \n",
    "        \n",
    "\n",
    "        # HAS-PROPERTY\n",
    "        \n",
    "        # Destination Node\n",
    "        hasPropertyIsd = doc.get('isd').get('/r/RelatedTo')\n",
    "        \n",
    "        # Checks if Destination Node is empty\n",
    "        if(hasPropertyIsd is not None):\n",
    "            #hasPropertyEn.append(list(filter(r.match, hasProperty)))\n",
    "            for featureIsd in hasPropertyIsd:\n",
    "               \n",
    "                if(filterStr in featureIsd):\n",
    "                    retrieved_feature = featureIsd.split('/')\n",
    "                    \n",
    "                    if(retrieved_feature[3] in nouns):\n",
    "                        emotionfeatures['destination_features'].append(retrieved_feature[3])\n",
    "       \n",
    "        \n",
    "        \n",
    "        # Source Node\n",
    "        hasPropertyIss = doc.get('iss').get('/r/RelatedTo')\n",
    "        \n",
    "        # Checks if Source Node is empty\n",
    "        if(hasPropertyIss is not None):\n",
    "            #hasPropertyEn.append(list(filter(r.match, hasProperty)))\n",
    "            for featureIss in hasPropertyIss:\n",
    "                if(filterStr in featureIss):\n",
    "                    \n",
    "                    retrieved_feature = featureIss.split('/')\n",
    "                    if(retrieved_feature[3] in nouns):\n",
    "                        emotionfeatures['source_features'].append(retrieved_feature[3])\n",
    "                    \n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    " \n",
    "print('\\nRetrieved Property:', emotionfeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f82b33",
   "metadata": {},
   "source": [
    "# Query ConceptNet to retrieve features of the 2nd Category set by User."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94278174",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplarfeatures = dict(second_exemplar=exemplar, destination_features=[], source_features=[])\n",
    "print(exemplarfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe092c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Pattern to only retrieve English Elements\n",
    "filterStr = '/c/en/'\n",
    "\n",
    "# Create the query with proper format w/ the Negative_Feature retrieved.\n",
    "query = query_format + exemplar\n",
    "print('\\n',query)\n",
    "    \n",
    "# Pre-Processing below does Stemming for ConceptNet. Basically if the word doesn't exist, remove a character.\n",
    "cursor = nodes.find({\"_id\": query})\n",
    "\n",
    "if(len(cursor[0]) > 0):\n",
    "    \n",
    "    # In the case there is no destination node, keep deleting characters and search again until there is a destination node.\n",
    "    while('isd' not in cursor[0]):\n",
    "        query = query[:-1]\n",
    "        cursor = nodes.find({\"_id\": query})\n",
    "        if(len(query) == 0):\n",
    "            break\n",
    "\n",
    "    print(cursor)\n",
    "\n",
    "    # compile regular expression to match any query with '/c/en' when querying MongoDB\n",
    "    r = re.compile(\".*/c/en/\")\n",
    "\n",
    "    # Retrieve the relations & features.\n",
    "    for doc in cursor: \n",
    "        \n",
    "\n",
    "        # HAS-PROPERTY\n",
    "        \n",
    "        # Destination Node\n",
    "        hasPropertyIsd = doc.get('isd').get('/r/RelatedTo')\n",
    "        \n",
    "        # Checks if Destination Node is empty\n",
    "        if(hasPropertyIsd is not None):\n",
    "            #hasPropertyEn.append(list(filter(r.match, hasProperty)))\n",
    "            for featureIsd in hasPropertyIsd:\n",
    "               \n",
    "                if(filterStr in featureIsd):\n",
    "                    \n",
    "                    retrieved_feature = featureIsd.split('/')\n",
    "                    if(retrieved_feature[3] in nouns):\n",
    "                        exemplarfeatures['destination_features'].append(retrieved_feature[3])\n",
    "       \n",
    "        \n",
    "        \n",
    "        # Source Node\n",
    "        hasPropertyIss = doc.get('iss').get('/r/RelatedTo')\n",
    "        \n",
    "        # Checks if Source Node is empty\n",
    "        if(hasPropertyIss is not None):\n",
    "            #hasPropertyEn.append(list(filter(r.match, hasProperty)))\n",
    "            for featureIss in hasPropertyIss:\n",
    "                if(filterStr in featureIss):\n",
    "                    \n",
    "                    retrieved_feature = featureIss.split('/')\n",
    "                    if(retrieved_feature[3] in nouns):\n",
    "                        exemplarfeatures['source_features'].append(retrieved_feature[3])\n",
    "        \n",
    "        \n",
    " \n",
    "print('\\nRetrieved Property:', exemplarfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a01de3",
   "metadata": {},
   "source": [
    "# Fuzzy Matching Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Set by User.\n",
    "\n",
    "print('Exemplar to compare to:', exemplar)\n",
    "print('Input-Sentence', token_sentence)\n",
    "print('Emotion Detected:', predicted_emotion['label'])\n",
    "print('No of Features to Compare:', criterionQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b3d3f",
   "metadata": {},
   "source": [
    "# Collect all the Properties for Fuzzy Matching of each Exemplar of retrieved Emotion Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects all the features of each em_feature\n",
    "em_features_collection = []\n",
    "\n",
    "all_emotion_features = emotionfeatures['destination_features'] + emotionfeatures['source_features']\n",
    "\n",
    "#print(emotionfeatures['destination_features'])\n",
    "#print(emotionfeatures['source_features'])\n",
    "\n",
    "#Loop through each feature of the emotion\n",
    "for em_feature in all_emotion_features:\n",
    "    \n",
    "    # Dict for em_feature\n",
    "    em_features = dict(emotion_feature = em_feature, feature_properties = [])\n",
    "    \n",
    "    \n",
    "    # Retrieve features of said em_feature\n",
    "    query = '/c/en/' + em_feature\n",
    "\n",
    "    # Pre-Processing below does Stemming for ConceptNet. Basically if the word doesn't exist, remove a character.\n",
    "    cursor = nodes.find({\"_id\": query})\n",
    "    \n",
    "    # Checks if cursor is not empty\n",
    "    if(len(cursor[0]) > 0):\n",
    "\n",
    "\n",
    "            # Destination Node 'isd'\n",
    "            \n",
    "            # Checks if '/r/RelatedTo' exists in the node\n",
    "            \n",
    "            # Checks if Destination Node exists\n",
    "            if('isd' in cursor[0]):\n",
    "                em_feature_isd = cursor[0]['isd']\n",
    "            \n",
    "                if ('/r/RelatedTo' in em_feature_isd.keys() and em_feature_isd is not None):\n",
    "                    hasPropertyIsd = em_feature_isd.get('/r/RelatedTo')\n",
    "\n",
    "                    # Checks if Destination Node is empty\n",
    "                    if(hasPropertyIsd is not None):\n",
    "\n",
    "                        for featureIsd in hasPropertyIsd:\n",
    "\n",
    "                            if(filterStr in featureIsd):\n",
    "                                \n",
    "                                retrieved_feature = featureIsd.split('/')\n",
    "                                \n",
    "                                # Check if retrieved feature is part of dictionary.\n",
    "                                #if(retrieved_feature[3] in words.words()):\n",
    "                                if(retrieved_feature[3] in nouns):\n",
    "                                    em_features['feature_properties'].append(retrieved_feature[3])\n",
    "                            \n",
    "            \n",
    "            # Checks if Source Node exists.\n",
    "            if('iss' in cursor[0]):\n",
    "                em_feature_iss = cursor[0]['iss']\n",
    "                if('/r/RelatedTo' in em_feature_iss.keys() and em_feature_iss is not None):\n",
    "                    hasPropertyIss = em_feature_iss.get('/r/RelatedTo')\n",
    "\n",
    "                    # Checks if Source Node is empty\n",
    "                    if(hasPropertyIss is not None):\n",
    "\n",
    "                        for featureIss in hasPropertyIss:\n",
    "\n",
    "                            if(filterStr in featureIss):\n",
    "                                \n",
    "                                retrieved_feature = featureIss.split('/')\n",
    "                                if(retrieved_feature[3] in nouns):\n",
    "                                    em_features['feature_properties'].append(retrieved_feature[3])\n",
    "\n",
    "                \n",
    "            em_features_collection.append(em_features)\n",
    "\n",
    "\n",
    "print(len(em_features_collection))\n",
    "print(em_features_collection)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feea495",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in em_features_collection:\n",
    "    print(i['emotion_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710aff38",
   "metadata": {},
   "source": [
    "# Collect all the Properties for Fuzzy Matching of each Exemplar of retrieved User-Input Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d87e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects all the features of each ex_feature\n",
    "ex_features_collection = []\n",
    "\n",
    "all_exemplar_features = exemplarfeatures['destination_features'] + exemplarfeatures['source_features']\n",
    "\n",
    "#Loop through each feature of the emotion\n",
    "for ex_feature in all_exemplar_features:\n",
    "    \n",
    "    # Dict for ex_feature\n",
    "    ex_features = dict(exemplar_feature = ex_feature, feature_properties = [])\n",
    "    \n",
    "    \n",
    "    # Retrieve features of said em_feature\n",
    "    query = '/c/en/' + ex_feature\n",
    "\n",
    "    # Pre-Processing below does Stemming for ConceptNet. Basically if the word doesn't exist, remove a character.\n",
    "    cursor = nodes.find({\"_id\": query})\n",
    "    \n",
    "    \n",
    "    # Checks if cursor is not empty\n",
    "    if(len(cursor[0]) > 0):\n",
    "\n",
    "            # Destination Node 'isd'\n",
    "            # Checks if Destination Node exists\n",
    "            if('isd' in cursor[0]):\n",
    "                ex_feature_isd = cursor[0]['isd']\n",
    "                # Checks if '/r/RelatedTo' exists in the node\n",
    "\n",
    "                if ('/r/RelatedTo' in ex_feature_isd.keys() and ex_feature_isd is not None):\n",
    "                    hasPropertyIsd = ex_feature_isd.get('/r/RelatedTo')\n",
    "\n",
    "                    # Checks if Destination Node is empty\n",
    "                    if(hasPropertyIsd is not None):\n",
    "\n",
    "                        for featureIsd in hasPropertyIsd:\n",
    "\n",
    "                            if(filterStr in featureIsd):\n",
    "                                \n",
    "                                retrieved_feature = featureIsd.split('/')\n",
    "                                if(retrieved_feature[3] in nouns):\n",
    "                                    ex_features['feature_properties'].append(retrieved_feature[3])\n",
    "                            \n",
    "                \n",
    "            # Checks if Source Node exists.\n",
    "            if('iss' in cursor[0]):\n",
    "                ex_feature_iss = cursor[0]['iss']\n",
    "                \n",
    "                if('/r/RelatedTo' in ex_feature_iss.keys() and ex_feature_iss is not None):\n",
    "                    hasPropertyIss = ex_feature_iss.get('/r/RelatedTo')\n",
    "\n",
    "                    # Checks if Source Node is empty\n",
    "                    if(hasPropertyIss is not None):\n",
    "\n",
    "                        for featureIss in hasPropertyIss:\n",
    "\n",
    "                            if(filterStr in featureIss):\n",
    "                                \n",
    "                                retrieved_feature = featureIss.split('/')\n",
    "                                if(retrieved_feature[3] in nouns):\n",
    "                                    ex_features['feature_properties'].append(retrieved_feature[3])\n",
    "                \n",
    "                \n",
    "            ex_features_collection.append(ex_features)\n",
    "\n",
    "\n",
    "print(len(ex_features_collection))\n",
    "print(ex_features_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc248b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ex_features_collection:\n",
    "    print(i['exemplar_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b43ade",
   "metadata": {},
   "source": [
    "# Fuzzy Match w/ Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2941893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(matrix, tokens):\n",
    "\n",
    "    doc_names = ['em_features', 'ex_features']\n",
    "    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0da78a",
   "metadata": {},
   "source": [
    "# Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfid_vectorize_cosine(featurelist1, featurelist2):\n",
    "    \n",
    "    Tfidf_vect = TfidfVectorizer()\n",
    "    \n",
    "    # Stringify List to String to vectorize\n",
    "    emotion_features_str = ' '.join(featurelist1)\n",
    "    exemplar_features_str = ' '.join(featurelist2)\n",
    "    \n",
    "    # Convert to list to vectorize\n",
    "    data = [emotion_features_str, exemplar_features_str]\n",
    "    \n",
    "    vector_matrix = Tfidf_vect.fit_transform(data)\n",
    "    \n",
    "     # Convert Vector Matrix to arrayc\n",
    "    vector_matrix.toarray()\n",
    "\n",
    "\n",
    "    # Generate Cosine Similarity score\n",
    "    cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "    \n",
    "    \n",
    "    return cosine_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e58c2",
   "metadata": {},
   "source": [
    "# Retrieve Cosine Similarity Score of all Associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the scores\n",
    "scoreList = []\n",
    "\n",
    "\n",
    "\n",
    "# Collect the Cosine Similarity Scores of each Association\n",
    "for em_feature in em_features_collection:\n",
    "    print(em_feature['emotion_feature'])\n",
    "    \n",
    "    for ex_feature in ex_features_collection:\n",
    "        \n",
    "        # Pass Feature Properties to compare in Cosine Similarity\n",
    "        \n",
    "        # Checks if Feature Properties are not empty. If empty no association can be made..\n",
    "        if(ex_feature['feature_properties'] and em_feature['feature_properties']):\n",
    "            cos_similarity = tfid_vectorize_cosine(em_feature['feature_properties'], ex_feature['feature_properties'])\n",
    "        \n",
    "            # Append to scorelist\n",
    "            scoreList.append(dict(emotion_feature = em_feature['emotion_feature'], exemplar_feature = ex_feature['exemplar_feature'], emotional_properties = em_feature['feature_properties'], exemplar_properties =  ex_feature['feature_properties'], score = cos_similarity))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844ed12",
   "metadata": {},
   "source": [
    "# HeapSort to find most similar Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python program for implementation of heap Sort\n",
    " \n",
    "# To heapify subtree rooted at index i.\n",
    "# n is size of heap\n",
    " \n",
    " \n",
    "def heapifyCosine(arr, n, i):\n",
    "    largest = i  # Initialize largest as root\n",
    "    l = 2 * i + 1     # left = 2*i + 1\n",
    "    r = 2 * i + 2     # right = 2*i + 2\n",
    " \n",
    "    # See if left child of root exists and is\n",
    "    # greater than root\n",
    "    if l < n and arr[largest]['score'][0][1] < arr[l]['score'][0][1]:\n",
    "        largest = l\n",
    " \n",
    "    # See if right child of root exists and is\n",
    "    # greater than root\n",
    "    if r < n and arr[largest]['score'][0][1] < arr[r]['score'][0][1]:\n",
    "        largest = r\n",
    " \n",
    "    # Change root, if needed\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]  # swap\n",
    " \n",
    "        # Heapify the root.\n",
    "        heapifyCosine(arr, n, largest)\n",
    " \n",
    "# The main function to sort an array of given size\n",
    " \n",
    " \n",
    "def heapSortCosine(arr):\n",
    "    n = len(arr)\n",
    " \n",
    "    # Build a maxheap.\n",
    "    for i in range(n//2 - 1, -1, -1):\n",
    "        heapifyCosine(arr, n, i)\n",
    " \n",
    "    # One by one extract elements\n",
    "    for i in range(n-1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]  # swap\n",
    "        heapifyCosine(arr, i, 0)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e072a",
   "metadata": {},
   "source": [
    "# Retrieve Largest Scoring Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call HeapSort for O(nlogn) time Complexity\n",
    "heapSortCosine(scoreList)\n",
    "\n",
    "# List to collect all combinations\n",
    "combinations = []\n",
    "\n",
    "# Index to determine whether CriterionQ has been met\n",
    "criterIndex = 0\n",
    "\n",
    "# Retrieve the Top Criterion Q Highest Scoring Combinations \n",
    "for combination in reversed(scoreList):\n",
    "    if(criterIndex == criterionQ):\n",
    "        break\n",
    "    \n",
    "    if(combination['score'][0][1] < 1):\n",
    "        criterIndex = criterIndex + 1\n",
    "        combinations.append(combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ceb70",
   "metadata": {},
   "source": [
    "# Print out all Combinations according to Criterion (Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_emotion['label'], 'and', exemplar)\n",
    "print('\\n')\n",
    "\n",
    "for metaphor in combinations:\n",
    "    print(metaphor['emotion_feature'], 'is', metaphor['exemplar_feature'], '\\nscore:', metaphor['score'][0][1])\n",
    "    print( '\\n Matching Properties between Exemplar & Emotion:', set(metaphor['exemplar_properties']) & set(metaphor['emotional_properties']) )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a6da7",
   "metadata": {},
   "source": [
    "# Fuzzy Match w/ Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6ec4b",
   "metadata": {},
   "source": [
    "# Jaccard Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(doc1, doc2): \n",
    "    \n",
    "    # List the unique words in a document\n",
    "    words_doc1 = set(doc1) \n",
    "    words_doc2 = set(doc2)\n",
    "    \n",
    "    # Find the intersection of words list of doc1 & doc2\n",
    "    intersection = words_doc1.intersection(words_doc2)\n",
    "\n",
    "    # Find the union of words list of doc1 & doc2\n",
    "    union = words_doc1.union(words_doc2)\n",
    "        \n",
    "    # Calculate Jaccard similarity score \n",
    "    # using length of intersection set divided by length of union set\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793fb053",
   "metadata": {},
   "source": [
    "# Heapsort edited w/ Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program for implementation of heap Sort\n",
    " \n",
    "# To heapify subtree rooted at index i.\n",
    "# n is size of heap\n",
    " \n",
    " \n",
    "def heapifyJaccard(arr, n, i):\n",
    "    largest = i  # Initialize largest as root\n",
    "    l = 2 * i + 1     # left = 2*i + 1\n",
    "    r = 2 * i + 2     # right = 2*i + 2\n",
    " \n",
    "    # See if left child of root exists and is\n",
    "    # greater than root\n",
    "    if l < n and arr[largest]['score'] < arr[l]['score']:\n",
    "        largest = l\n",
    " \n",
    "    # See if right child of root exists and is\n",
    "    # greater than root\n",
    "    if r < n and arr[largest]['score'] < arr[r]['score']:\n",
    "        largest = r\n",
    " \n",
    "    # Change root, if needed\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]  # swap\n",
    " \n",
    "        # Heapify the root.\n",
    "        heapifyJaccard(arr, n, largest)\n",
    " \n",
    "# The main function to sort an array of given size\n",
    " \n",
    " \n",
    "def heapSortJaccard(arr):\n",
    "    n = len(arr)\n",
    " \n",
    "    # Build a maxheap.\n",
    "    for i in range(n//2 - 1, -1, -1):\n",
    "        heapifyJaccard(arr, n, i)\n",
    " \n",
    "    # One by one extract elements\n",
    "    for i in range(n-1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]  # swap\n",
    "        heapifyJaccard(arr, i, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3258a94",
   "metadata": {},
   "source": [
    "# Retrieve Jaccard Score for each Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the scores\n",
    "scoreListJaccard = []\n",
    "\n",
    "\n",
    "\n",
    "# Collect the Cosine Similarity Scores of each Association\n",
    "for em_feature in em_features_collection:\n",
    "    print(em_feature['emotion_feature'])\n",
    "    \n",
    "    for ex_feature in ex_features_collection:\n",
    "        \n",
    "        # Pass Feature Properties to compare in Cosine Similarity\n",
    "        \n",
    "        # Checks if Feature Properties are not empty. If empty no association can be made..\n",
    "        if(ex_feature['feature_properties'] and em_feature['feature_properties']):\n",
    "            jacc_similarity = jaccard_similarity(em_feature['feature_properties'], ex_feature['feature_properties'])\n",
    "        \n",
    "            # Append to scorelist\n",
    "            scoreListJaccard.append(dict(emotion_feature = em_feature['emotion_feature'], exemplar_feature = ex_feature['exemplar_feature'], emotional_properties = em_feature['feature_properties'], exemplar_properties =  ex_feature['feature_properties'], score = jacc_similarity))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146caa15",
   "metadata": {},
   "source": [
    "# Retrieve Largest Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5544166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call HeapSort for O(nlogn) time Complexity\n",
    "heapSortJaccard(scoreListJaccard)\n",
    "\n",
    "# List to collect all combinations\n",
    "combinationsJaccard = []\n",
    "\n",
    "# Index to determine whether CriterionQ has been met\n",
    "criterIndex = 0\n",
    "\n",
    "# Retrieve the Top Criterion Q Highest Scoring Combinations \n",
    "for combination in reversed(scoreListJaccard):\n",
    "    if(criterIndex == criterionQ):\n",
    "        break\n",
    "    \n",
    "    if(combination['score'] < 1.0):\n",
    "        criterIndex = criterIndex + 1\n",
    "        combinationsJaccard.append(combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4378f",
   "metadata": {},
   "source": [
    "# Print out all combinations according to criterion (Jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7378da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_emotion['label'], 'and', exemplar)\n",
    "print('\\n')\n",
    "\n",
    "for metaphor in combinationsJaccard:\n",
    "    print(metaphor['emotion_feature'], 'is', metaphor['exemplar_feature'], '\\nscore:', metaphor['score'])\n",
    "    print( '\\n Matching Properties between Exemplar & Emotion:', set(metaphor['exemplar_properties']) & set(metaphor['emotional_properties']) )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceed5cc",
   "metadata": {},
   "source": [
    "# Additional Theory to choose\n",
    "\n",
    "Comparison between two approaches\n",
    "\n",
    "Take Average of all combinations\n",
    "\n",
    "Make additional theory explicit.\n",
    "\n",
    "Weighted Features\n",
    "\n",
    "Theory to explain Intersection\n",
    "\n",
    "Compare Knowledge Base with ConceptNet\n",
    "\n",
    "Association is weird.\n",
    "\n",
    "Crossover between Literals & Figurative features. Quality of all those features.\n",
    "\n",
    "Allow user to adjust settings of the system, select what is literal (downright description) & figurative (anything symbolic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bf78a",
   "metadata": {},
   "source": [
    "# Calculate Fuzzy Intersection, \n",
    "\n",
    "# Then look into relationships of LL FF LF FL\n",
    "\n",
    "# No pre-defining criteria, give User choice to choose.\n",
    "\n",
    "\n",
    "# Select based on Rank-Order (most LF, FL)\n",
    "\n",
    "# Take Chained Associations as a single line.\n",
    "\n",
    "# Spearman Rank Order Test\n",
    "\n",
    "# From Prof Johan to Everyone 02:20 PM\n",
    "https://statistics.laerd.com/spss-tutorials/spearmans-rank-order-correlation-using-spss-statistics.php#:~:text=The%20Spearman%20rank%2Dorder%20correlation,letter%20%CF%81%2C%20pronounced%20rho).\n",
    "\n",
    "# Rank Metaphors from best to worst.\n",
    "\n",
    "# Selection Criteria User-Input (Threshold for )\n",
    "\n",
    "# Extension of ConceptNet to show how it actually works. \n",
    "# ConceptNet is the variable that should be improved. Impaired Conceptnet to show what it does.\n",
    "# Robotic Metaphor Production is the Theory - what we are implementing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930735e6",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "# Define Gambling Threshold (25%) (4 categories, 0.25 have it right can be gambling)\n",
    "\n",
    "# Add 1 point when more than 25% of people get the correct classificaiton of sentence. (every 7.5% added to 25%, 1 point)\n",
    "\n",
    "# Score ranging from 1 to 10\n",
    "\n",
    "# https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/sign-test/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e2f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
